{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Logistic Regression / Classification \n",
    "\n",
    "## Background\n",
    "Binary classification is one of the most fundamental and common tasks in machine learning and deep learning. \n",
    "At its core, it's a supervised learning problem where the goal is to categorize input data into one of two possible classes or categories. \n",
    "Think of it as teaching a machine to make a \"yes\" or \"no\" decision.\n",
    "Examples of binary classification's applications are:\n",
    "\n",
    "- Spam Detection: Classifying an email as either 'spam' or 'not spam'.\n",
    "- Medical Diagnosis: Determining if a patient has a certain disease.\n",
    "- Credit Scoring: Deciding whether a loan applicant is a 'good credit risk' or a 'bad credit risk'.\n",
    "\n",
    "A sigmoid activated multi-input, single-output linear model is a decent candidate for this job.\n",
    "In this practice, you'll train such a model to categorize samples from a toy dataset into two classes.\n",
    "\n",
    "## Objectives\n",
    "- Train a simple neural network model.\n",
    "    - Apply sigmoid activation on a linear model.\n",
    "    - Implement binary cross entropy evaluation.\n",
    "    - Practice vectorized computing.\n",
    "- Assess model's performance using a variety of metrics.\n",
    "\n",
    "\n",
    "## Exercises:\n",
    "\n",
    "1. <font color=#582C83>(5%) Data Pre-Processing</font>\n",
    "2. <font color=#582C83>(10%) Model Creation</font>\n",
    "3. <font color=#582C83>(20%) Model Assessment</font>\n",
    "4. <font color=#582C83>(10%) Gradient of BCE Loss</font>\n",
    "5. <font color=#582C83>(30%) Gradient Descent Model Optimization</font>\n",
    "6. <font color=#582C83>(25%) Classification Metrics</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Pre-Process Training Dataset\n",
    "\n",
    "The dataset, \n",
    "$$\\mathcal{D} = \\{\\mathbf{X}, \\mathbf{y}\\} = \\begin{bmatrix} ^{(1)} \\mathbf{x} & ^{(1)} y \\\\ ^{(2)} \\mathbf{x} & ^{(2)} y \\\\ \\vdots \\\\ ^{(M)} \\mathbf{x} & ^{(M)} y \\end{bmatrix}$$\n",
    "consists $M$ samples.\n",
    "\n",
    "Each sample has $N$ features: $^{(i)} \\mathbf{x} = [^{(i)} x_1, ^{(i)} x_2, \\dots, ^{(i)} x_N]$ and is labeled as $^{(i)} y \\in \\{0, 1\\}$, where $i \\in \\{1, 2, ..., M\\}$.\n",
    "\n",
    "In this practice, we make a toy dataset using [scikit-learn](https://scikit-learn.org/stable/index.html)'s [datasets](https://scikit-learn.org/stable/api/sklearn.datasets.html) module. \n",
    "The dataset has $M=2048$ samples, and $N=2$ features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "features_raw, labels_raw = make_blobs(n_samples=2048, centers=np.array([[5, 5], [7, 8]]), cluster_std=1.0, random_state=3321)\n",
    "# Visualize the raw data\n",
    "plt.scatter(\n",
    "    features_raw[:, 0], # feature 1 values (x coords)\n",
    "    features_raw[:, 1], # feature 2 values (y coords)\n",
    "    s=5*np.ones(labels_raw.size), # marker size all set to 1\n",
    "    c=labels_raw, # colorized labels: 0:blue, 1:red\n",
    "    cmap='coolwarm'  # color map\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=#582C83>(5%) Exercise 1: Data Pre-Processing</font>\n",
    "1. In machine learning, it is prefered to scale features into the same range: $[0, 1]$.\n",
    "2. To avoid dimension mismatch, reshape features and labels array into two dimensions is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODING HERE ### (≈ 2 lines of code)\n",
    "features_train = None  # Scale features to [0, 1]\n",
    "labels_train = None # Reshape labels to be a column vector\n",
    "### END CODING HERE ### \n",
    "\n",
    "# Sanity check\n",
    "print(f\"Samples of pre-processed features:\\n {features_train[-4:]}\")\n",
    "print(f\"Samples of pre-processed labels:\\n {labels_train[-4:]}\")  # Print last 4 samples of features and labels\n",
    "print(f\"Shape of pre-processed features:\\n {features_train.shape}.\")\n",
    "print(f\"Shape of pre-processed labels:\\n {labels_train.shape}\")  # Print shapes of features and labels   \n",
    "\n",
    "# Visualize\n",
    "plt.scatter(\n",
    "    features_train[:, 0], \n",
    "    features_train[:, 1], \n",
    "    s=5*np.ones(labels_train.size), \n",
    "    c=labels_train, \n",
    "    cmap='coolwarm'  # color map\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Outcomes**:\n",
    "```console\n",
    "Samples of pre-processed features:\n",
    " [[0.60810041 0.36828791]\n",
    " [0.85139165 0.63786153]\n",
    " [0.57346525 0.38949334]\n",
    " [0.77146837 0.71406137]]\n",
    "Samples of pre-processed labels:\n",
    " [[0]\n",
    " [1]\n",
    " [0]\n",
    " [1]]\n",
    "Shape of pre-processed features:\n",
    " (2048, 2).\n",
    "Shape of pre-processed labels:\n",
    " (2048, 1)\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a Non-linear Multi-Input Single-Output Model\n",
    "Let's create a nonlinear model by applying a sigmoid function to the output of a linear model.\n",
    "\n",
    "$$\\mathbf{\\hat{y}} = \\sigma (\\mathbf{X} \\cdot \\mathbf{w}^T + \\mathbf{b}) = \\sigma(\\mathbf{Z})$$\n",
    "\n",
    "The sigmoid function can be represented as,\n",
    "\n",
    "$$\\sigma(\\mathbf{Z}) = \\frac{1}{1 + e^{-\\mathbf{Z}}}$$\n",
    "\n",
    "The linear part of the model is governed by a parameter vector \n",
    "$$\\mathbf{w} = [w_1, w_2, ..., w_N]_{(1, N)}$$\n",
    "\n",
    "and a bias vector \n",
    "$$\\mathbf{b} = \\begin{bmatrix} b \\\\ b \\\\ \\vdots \\\\ b \\end{bmatrix}_{(M, 1)}$$\n",
    "\n",
    "And, in this practice $M=2048$ and $N=2$\n",
    "\n",
    "### <font color=#582C83>(10%) Exercise 2: Model Creation</font>\n",
    "Let's translate math to code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(in_features, weights, bias):\n",
    "    \"\"\"\n",
    "    Compute the linear combination of inputs and weights plus bias.\n",
    "    \n",
    "    Args:\n",
    "        in_features (np.ndarray): Input features of shape (M, N).\n",
    "        weights (np.ndarray): Weights of shape (1, N).\n",
    "        bias (float): Bias term.  \n",
    "    Returns:\n",
    "        predictions: Linear combination result of shape (M, 1).\n",
    "    \"\"\"\n",
    "    ### START CODING HERE ### (≈ 2 lines of code)\n",
    "    def sigmoid(z):\n",
    "        return None\n",
    "    predictions = None\n",
    "    return predictions\n",
    "    ### END CODING HERE ###\n",
    "\n",
    "# Sanity check\n",
    "w_dummy = np.array([[1.2, -2.3]])\n",
    "b_dummy = 1.0\n",
    "print(f\"Samples of dummy prediction:\\n {forward(features_train, w_dummy, b_dummy)[-4:]}\")  # Print last 4 samples of dummy prediction\n",
    "\n",
    "# Visualize dummy model decision boundary\n",
    "plt.scatter(features_train[:, 0], features_train[:, 1], s=5*np.ones(labels_train.size), c=labels_train, cmap='coolwarm')\n",
    "# Create a meshgrid of points over the feature space\n",
    "xx, yy = np.meshgrid(np.linspace(0., 1., 300),\n",
    "                     np.linspace(0., 1., 300))\n",
    "zz = w_dummy[0, 0] * xx + w_dummy[0, 1] * yy + b_dummy - 0.5\n",
    "plt.contourf(xx, yy, zz, levels=[-np.inf, 0, np.inf], colors=['blue', 'red'], alpha=0.25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Outcomes**:\n",
    ">\n",
    "```console\n",
    "Samples of dummy prediction:\n",
    " [[0.70737278]\n",
    " [0.63519949]\n",
    " [0.68833004]\n",
    " [0.5703847 ]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate Model\n",
    "As same as the linear regression, we need to evaluate model performance. \n",
    "It is OK to use a Mean Squared Error (MSE) function to compute the model loss. \n",
    "It is **recommended** to use a Binary Cross Entropy (BCE) function to assess the model for a binary classification problem. \n",
    "Binary Cross Entropy (BCE) is a differentiable metric.\n",
    "$$\\mathcal{L}(\\mathbf{\\hat{y}}, \\mathbf{y}) = \\frac{1}{M} \\Sigma [-\\mathbf{y} \\log \\hat{\\mathbf{y}} - (1 - \\mathbf{y}) \\log(1 - \\hat{\\mathbf{y}})]$$\n",
    "\n",
    "There are other useful metrics, such as accuracy. But they are not differentiable.\n",
    "Let $TP$ be number of true positive predictions, $TN$ be number of true negative predictions, $FP$ be number of false positive predictions, $FN$ be number of false negative predictions.\n",
    "The accuracy can be calculated by:\n",
    "$$Accuracy = \\frac{TP + TN}{TP+FP+TN+FN}$$\n",
    "\n",
    "### <font color=#582C83>(20%) Exercise 3: Model Assessment</font>\n",
    "1. Evaluate model loss with BCE.\n",
    "2. Assess model predictions accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_loss(predictions, labels):\n",
    "    \"\"\"\n",
    "    Compute the Binary Cross Entropy (BCE) loss.\n",
    "    \n",
    "    Args:\n",
    "        predictions (np.ndarray): Predicted probabilities of shape (M, 1).\n",
    "        labels (np.ndarray): True binary labels of shape (M, 1).\n",
    "    Returns:\n",
    "        loss: BCE loss value (float).\n",
    "    \"\"\"\n",
    "    ### START CODING HERE ### (≈ 1 line)\n",
    "    loss = None\n",
    "    return loss\n",
    "    ### END CODING HERE ###\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    \"\"\"\n",
    "    Compute the accuracy of predictions.\n",
    "    \n",
    "    Args:\n",
    "        predictions (np.ndarray): Predicted probabilities of shape (M, 1).\n",
    "        labels (np.ndarray): True binary labels of shape (M, 1).\n",
    "    Returns:\n",
    "        acc: Accuracy value (float).\n",
    "    \"\"\"\n",
    "    ### START CODING HERE ### (≈ 2 lines)\n",
    "    predicted_classes = None  # Convert probabilities to binary classes\n",
    "    acc = None\n",
    "    return acc\n",
    "    ### END CODING HERE ###\n",
    "\n",
    "# Sanity check\n",
    "preds_dummy = forward(features_train, w_dummy, b_dummy)\n",
    "print(f\"Dummy model BCE loss: {bce_loss(preds_dummy, labels_train)}\")\n",
    "print(f\"Dummy model accuracy: {accuracy(preds_dummy, labels_train) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    ">\n",
    "```console\n",
    "Dummy model BCE loss: 0.8189666983092332\n",
    "Dummy model accuracy: 42.67578125%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute Gradient of Loss\n",
    "Compute gradient of the BCE loss:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}} = \\begin{bmatrix} \\frac{\\partial \\mathcal{L}}{\\partial w_1} & \\frac{\\partial \\mathcal{L}}{\\partial w_2} & \\dots & \\frac{\\partial \\mathcal{L}}{\\partial w_N} \\end{bmatrix} = \\frac{1}{M} (\\hat{\\mathbf{y}} - \\mathbf{y})^T \\cdot \\mathbf{X}$$\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial b}  = \\overline{\\hat{\\mathbf{y}} - \\mathbf{y}} $$\n",
    "\n",
    "### <font color=#582C83>(10%) Exercise 4: Gradient of BCE Loss</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(predictions, labels, in_features):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the BCE loss with respect to weights and bias.\n",
    "    \n",
    "    Args:\n",
    "        predictions (np.ndarray): Predicted probabilities of shape (M, 1).\n",
    "        labels (np.ndarray): True binary labels of shape (M, 1).\n",
    "        in_features (np.ndarray): Input features of shape (M, N).\n",
    "    Returns:\n",
    "        dw: Gradient with respect to weights of shape (1, N).\n",
    "        db: Gradient with respect to bias (float).\n",
    "    \"\"\"\n",
    "    ### START CODING HERE ### (≈ 3 lines of code)\n",
    "    M = None  # Number of samples\n",
    "    dw = None  # Gradient w.r.t. weights\n",
    "    db = None  # Gradient w.r.t. bias\n",
    "    return dw, db\n",
    "    ### END CODING HERE ###\n",
    "\n",
    "# Sanity check\n",
    "dw_dummy, db_dummy = gradient(preds_dummy, labels_train, features_train)\n",
    "print(f\"Dummy model gradient w.r.t. weights: {dw_dummy}, shape: {dw_dummy.shape}\")\n",
    "print(f\"Dummy model gradient w.r.t. bias: {db_dummy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    ">\n",
    "```console\n",
    "Dummy model gradient w.r.t. weights: [[ 0.01011904 -0.01882587]], shape: (1, 2)\n",
    "Dummy model gradient w.r.t. bias: 0.10192657501589689\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Update Model Parameters\n",
    "\n",
    "\n",
    "By tweaking the model parameters along the gradient a small step (learning rate, $\\alpha$) iteratively, we expect to bring the BCE loss down to a reasonable scale.\n",
    "\n",
    "- $\\text{Initialize } \\mathbf{w} \\text{ and } b$\n",
    "- $\\text{Repeat until converge}$\n",
    "    - $\\mathbf{w} = \\mathbf{w} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}}$\n",
    "    - $b = b - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial b}$\n",
    "\n",
    "    \n",
    "\n",
    "### <font color=#582C83>(30%) Exercise 5: Gradient Descent Model Optimization</font>\n",
    "1. Initialize weights and bias\n",
    "2. Set learning rate and number of iterations\n",
    "3. Repeat following steps:\n",
    "\n",
    "    1. Make predictions\n",
    "    2. Assess model\n",
    "    3. Compute gradient of loss\n",
    "    4. Update weights and bias\n",
    "\n",
    "<font color=red>**Note: bring training accuracy over 95% within 10000 iterations**</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START CODING HERE ### (≈ 10 lines of code)\n",
    "# Initialize parameters\n",
    "w = None  # Initialize weights\n",
    "b = None  # Initialize bias\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = None\n",
    "num_iters = None\n",
    "\n",
    "# Reserve loss and accuracy storage\n",
    "losses_train = []\n",
    "accuracies_train = []\n",
    "\n",
    "# Training loop\n",
    "for i in range(num_iters):\n",
    "    preds_train = None  # Make predictions\n",
    "    loss_train = None  # Assess model loss\n",
    "    acc_train = None  # Assess model accuracy\n",
    "    print(f\"Iteration {i+1}/{num_iters}, Loss: {loss_train:.4f}, Accuracy: {acc_train*100:.2f}%\")\n",
    "    losses_train.append(loss_train)\n",
    "    accuracies_train.append(acc_train)\n",
    "    dw, db = None  # Compute gradient of loss\n",
    "    w = None  # Update weights\n",
    "    b = None  # Update bias\n",
    "\n",
    "### END CODING HERE ###\n",
    "\n",
    "# Plot training loss and accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses_train, label='Training Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('BCE Loss')\n",
    "plt.title('Training Loss over Iterations')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.array(accuracies_train) * 100, label='Training Accuracy', color='orange')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training Accuracy over Iterations')\n",
    "plt.legend()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize decision boundary after training\n",
    "plt.scatter(features_train[:, 0], features_train[:, 1], s=5*np.ones(labels_train.size), c=labels_train, cmap='coolwarm')\n",
    "xx, yy = np.meshgrid(np.linspace(0., 1., 300),\n",
    "                     np.linspace(0., 1., 300))\n",
    "zz = w[0, 0] * xx + w[0, 1] * yy + b - 0.5\n",
    "plt.contourf(xx, yy, zz, levels=[-np.inf, 0, np.inf], colors=['blue', 'red'], alpha=0.25)\n",
    "plt.title('Decision Boundary after Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Model with a New Dataset\n",
    "Let's generate a similar dataset using [make_blobs](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html#sklearn.datasets.make_blobs) as used in the beginning of this practice.\n",
    "Make predictions and evaluate model with this newly generated dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy on training data\n",
    "X_raw, y_raw = make_blobs(n_samples=512, centers=np.array([[5, 5], [7, 8]]), cluster_std=1.0,)\n",
    "features_test = X_raw / X_raw.max(axis=0)  # Scale features to [-1, 1]\n",
    "labels_test = y_raw.reshape(-1, 1)  # Reshape labels to be a column vector\n",
    "\n",
    "# Visualize test data\n",
    "plt.scatter(\n",
    "    features_test[:, 0], \n",
    "    features_test[:, 1], \n",
    "    s=5*np.ones(labels_test.size), \n",
    "    c=labels_test, \n",
    "    cmap='managua'  # color map\n",
    ")\n",
    "# Visualize decision boundary on test data\n",
    "plt.contourf(xx, yy, zz, levels=[-np.inf, 0, np.inf], colors=['orange', 'cyan'], alpha=0.25)\n",
    "plt.title('Decision Boundary on Test Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides accuracy, a handful of metrics are useful to evaluate how sensitive/precise/balance the model is. \n",
    "A confusion matrix is a handy tool to visualize these metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for Test Data\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "preds_test = forward(features_test, w, b)\n",
    "cm_test = confusion_matrix(labels_test, (preds_test >= 0.5).astype(int))\n",
    "disp_test = ConfusionMatrixDisplay(confusion_matrix=cm_test, display_labels=[0, 1])\n",
    "disp_test.plot(cmap='PuRd')\n",
    "plt.title('Confusion Matrix - Test Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=#582C83>(25%) Exercise 6: Classification Metrics</font>\n",
    "According to your confusion matrix on the **test dataset**. Please calculate the following metrics and write your answers below.\n",
    "> Let $0$ be negative, and $1$ be positive.\n",
    "\n",
    "- `Accuracy`=?\n",
    "- `Recall`=?\n",
    "- `Precision`=?\n",
    "- `F1 Score`=?\n",
    "\n",
    "<font color=#582C83>Assume the model is used by the quality control department in a factory to detect defective products (class 1).\n",
    "Please briefly state (2 sentences maximum) why or why not this model is good for this job.</font>\n",
    "> Please write down your answer below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations! You have finished this assignment!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
